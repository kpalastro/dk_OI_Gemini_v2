# Import CSV Files to Database
# Imports CSV files generated by export_to_csv.py back into the database
# Reads from the export folder and restores data to all tables

import pandas as pd
from datetime import datetime
import logging
import os
from pathlib import Path

from time_utils import now_ist
from database_new import get_db_connection, release_db_connection, _get_placeholder, get_config

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(message)s')

EXPORT_DIR = Path("export")

def find_csv_files():
    """Find all CSV files in the export directory."""
    if not EXPORT_DIR.exists():
        print(f"ERROR: Export directory '{EXPORT_DIR}' does not exist!")
        return {}
    
    csv_files = {}
    for file_path in EXPORT_DIR.glob("*.csv"):
        filename = file_path.name
        
        # Map CSV files to table names
        if filename.startswith("option_chain_snapshots_today_"):
            csv_files["option_chain_snapshots"] = file_path
        elif filename.startswith("ml_features_"):
            csv_files["ml_features"] = file_path
        elif filename.startswith("exchange_metadata_"):
            csv_files["exchange_metadata"] = file_path
        elif filename.startswith("training_batches_"):
            csv_files["training_batches"] = file_path
        elif filename.startswith("vix_term_structure_"):
            csv_files["vix_term_structure"] = file_path
        elif filename.startswith("macro_signals_"):
            csv_files["macro_signals"] = file_path
        elif filename.startswith("order_book_depth_snapshots_"):
            csv_files["order_book_depth_snapshots"] = file_path
        # Skip database_summary_*.csv as it's not a table
    
    return csv_files

def clean_dataframe(df, table_name):
    """Clean and prepare dataframe for database insertion."""
    # Replace NaN with None for proper NULL handling
    df = df.where(pd.notnull(df), None)
    
    # Convert datetime columns if they exist
    datetime_columns = ['timestamp', 'created_at', 'updated_at', 'last_update_time', 
                       'start_timestamp', 'end_timestamp']
    for col in datetime_columns:
        if col in df.columns:
            # Try to parse datetime, keep as string if it fails
            try:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                # Convert to string format for database
                df[col] = df[col].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(x) else None)
            except:
                pass
    
    # Convert boolean columns for SQLite compatibility
    if table_name == "paper_trading_metrics":
        bool_columns = ['executed', 'constraint_violation']
        for col in bool_columns:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: 1 if x in (True, 1, '1', 'True', 'true') else 0 if x in (False, 0, '0', 'False', 'false') else None)
    
    return df

def get_table_columns(conn, table_name, config):
    """Get actual column names from the database table."""
    cursor = conn.cursor()
    is_postgres = config.db_type == 'postgres'
    
    try:
        if is_postgres:
            query = """
                SELECT column_name 
                FROM information_schema.columns 
                WHERE table_name = %s
                ORDER BY ordinal_position
            """
            cursor.execute(query, (table_name,))
            columns = [row[0] for row in cursor.fetchall()]
        else:
            # SQLite
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = [row[1] for row in cursor.fetchall()]
        
        return set(columns)
    except Exception as e:
        logging.warning(f"Could not fetch columns for {table_name}: {e}")
        return None

def import_table(conn, table_name, df, config):
    """Import a dataframe into a database table."""
    if df.empty:
        print(f"   ⚠ Skipping {table_name}: DataFrame is empty")
        return 0
    
    cursor = conn.cursor()
    ph = _get_placeholder()
    is_postgres = config.db_type == 'postgres'
    
    try:
        # Get actual database columns
        db_columns = get_table_columns(conn, table_name, config)
        if db_columns is None:
            print(f"   ⚠ Could not fetch database schema for {table_name}, using CSV columns")
            db_columns = set(df.columns)
        
        # Column name mappings for backward compatibility
        column_mappings = {
            'sentiment_score': 'news_sentiment_score',  # Old CSV might have this
        }
        
        # Map CSV column names to database column names
        csv_to_db = {}
        used_db_columns = set()  # Track which DB columns we've already mapped
        
        for csv_col in df.columns:
            if csv_col in db_columns:
                # CSV column directly matches DB column
                if csv_col not in used_db_columns:
                    csv_to_db[csv_col] = csv_col
                    used_db_columns.add(csv_col)
            elif csv_col in column_mappings:
                # CSV column needs mapping
                db_col = column_mappings[csv_col]
                if db_col in db_columns and db_col not in used_db_columns:
                    # Only map if target DB column doesn't already exist in CSV
                    if db_col not in df.columns:
                        csv_to_db[csv_col] = db_col
                        used_db_columns.add(db_col)
                        print(f"   ℹ Mapping CSV column '{csv_col}' to database column '{db_col}'")
                    else:
                        # Target column already exists in CSV, skip mapping
                        print(f"   ℹ Skipping mapping of '{csv_col}' - '{db_col}' already exists in CSV")
        
        # Filter to only columns that exist in database (or can be mapped)
        # Use set to deduplicate, then convert back to list to preserve order
        valid_columns = []
        seen = set()
        for csv_col in df.columns:
            if csv_col in csv_to_db:
                db_col = csv_to_db[csv_col]
                if db_col not in seen:
                    valid_columns.append(db_col)
                    seen.add(db_col)
        
        # Exclude id for auto-increment tables
        exclude_id = table_name in [
            "option_chain_snapshots", "training_batches", "vix_term_structure",
            "macro_signals", "order_book_depth_snapshots"
        ]
        
        if exclude_id and 'id' in valid_columns:
            valid_columns = [col for col in valid_columns if col != 'id']
        
        # Warn about columns that will be skipped
        skipped_columns = [col for col in df.columns if col not in csv_to_db and col != 'id']
        if skipped_columns:
            print(f"   ⚠ Skipping columns not in database: {', '.join(skipped_columns)}")
        
        if not valid_columns:
            print(f"   ⚠ Skipping {table_name}: No valid columns found")
            return 0
        
        # Create reverse mapping (db_col -> csv_col) for data extraction
        db_to_csv = {db_col: csv_col for csv_col, db_col in csv_to_db.items()}
        
        # Determine final columns to use (may exclude id for SQLite auto-increment)
        final_columns = valid_columns.copy()
        if not is_postgres and exclude_id and 'id' in final_columns:
            final_columns = [col for col in final_columns if col != 'id']
        
        # Prepare data rows using database column names
        rows = []
        for _, row in df.iterrows():
            row_data = tuple(row[db_to_csv[db_col]] if db_col in db_to_csv else None for db_col in final_columns)
            rows.append(row_data)
        
        if not rows:
            print(f"   ⚠ Skipping {table_name}: No rows to insert")
            return 0
        
        # Build INSERT statement with conflict resolution
        placeholders = ', '.join([ph] * len(final_columns))
        
        if is_postgres:
            # PostgreSQL: Use ON CONFLICT DO UPDATE
            # Define conflict columns based on table schema
            if table_name == "ml_features":
                conflict_cols = "(timestamp, exchange)"
            elif table_name == "exchange_metadata":
                conflict_cols = "(exchange)"
            elif table_name == "option_chain_snapshots":
                conflict_cols = "(timestamp, exchange, strike, option_type)"
            else:
                # For other tables, if id exists in final columns, use it for conflict
                # Otherwise, no conflict resolution (will create duplicates)
                if 'id' in final_columns and not exclude_id:
                    conflict_cols = "(id)"
                else:
                    conflict_cols = None
            
            if conflict_cols:
                update_clause = ", ".join([f"{col} = EXCLUDED.{col}" for col in final_columns])
                query = f"""
                    INSERT INTO {table_name} ({', '.join(final_columns)})
                    VALUES ({placeholders})
                    ON CONFLICT {conflict_cols}
                    DO UPDATE SET {update_clause}
                """
            else:
                # No conflict resolution - just insert (may create duplicates)
                query = f"""
                    INSERT INTO {table_name} ({', '.join(final_columns)})
                    VALUES ({placeholders})
                """
        else:
            # SQLite: Use INSERT OR REPLACE
            query = f"""
                INSERT OR REPLACE INTO {table_name} ({', '.join(final_columns)})
                VALUES ({placeholders})
            """
        
        # Execute batch insert
        cursor.executemany(query, rows)
        inserted_count = cursor.rowcount if hasattr(cursor, 'rowcount') and cursor.rowcount > 0 else len(rows)
        
        conn.commit()
        return inserted_count
        
    except Exception as e:
        conn.rollback()
        raise e

def import_all_data():
    """Import all CSV files from export folder into database."""
    
    config = get_config()
    db_name = config.db_name if config.db_type == 'postgres' else "db_new.db"
    
    print("=" * 70)
    print(f"CSV IMPORT TO DATABASE '{db_name}'")
    print("=" * 70)
    
    # Find CSV files
    csv_files = find_csv_files()
    
    if not csv_files:
        print(f"\n❌ No CSV files found in '{EXPORT_DIR}' directory!")
        print("   Make sure you have exported data using export_to_csv.py first.")
        return
    
    print(f"\nFound {len(csv_files)} CSV file(s) to import:")
    for table, file_path in csv_files.items():
        print(f"   - {table}: {file_path.name}")
    
    conn = None
    try:
        conn = get_db_connection()
        
        total_imported = 0
        
        # Import each table in order
        table_order = [
            "exchange_metadata",
            "vix_term_structure",
            "macro_signals",
            "order_book_depth_snapshots",
            "option_chain_snapshots",
            "ml_features",
            "training_batches"
        ]
        
        # Filter to only tables that have CSV files
        tables_to_import = [t for t in table_order if t in csv_files]
        
        for idx, table_name in enumerate(tables_to_import, 1):
            file_path = csv_files[table_name]
            print(f"\n{idx}. Importing {table_name}...")
            print(f"   Reading: {file_path.name}")
            
            try:
                # Read CSV
                df = pd.read_csv(file_path)
                print(f"   ✓ Loaded {len(df)} records from CSV")
                
                # Clean dataframe
                df = clean_dataframe(df, table_name)
                
                # Import to database
                imported_count = import_table(conn, table_name, df, config)
                total_imported += imported_count
                print(f"   ✓ Imported {imported_count} records to {table_name}")
                
            except Exception as e:
                print(f"   ❌ Error importing {table_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print("\n" + "=" * 70)
        print(f"✓ IMPORT COMPLETE! Total records imported: {total_imported}")
        print("=" * 70)
        
    except Exception as e:
        print(f"\n❌ AN ERROR OCCURRED: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if conn:
            release_db_connection(conn)

if __name__ == '__main__':
    import_all_data()

