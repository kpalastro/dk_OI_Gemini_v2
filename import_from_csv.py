# Import CSV Files to Database
# Imports CSV files generated by export_to_csv.py back into the database
# Reads from the export folder and restores data to all tables

import pandas as pd
from datetime import datetime
import logging
import os
from pathlib import Path

from time_utils import now_ist
from database_new import get_db_connection, release_db_connection, _get_placeholder, get_config

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(message)s')

EXPORT_DIR = Path("export")

def find_csv_files():
    """Find all CSV files in the export directory."""
    if not EXPORT_DIR.exists():
        print(f"ERROR: Export directory '{EXPORT_DIR}' does not exist!")
        return {}
    
    csv_files = {}
    for file_path in EXPORT_DIR.glob("*.csv"):
        filename = file_path.name
        
        # Map CSV files to table names
        if filename.startswith("option_chain_snapshots_today_"):
            csv_files["option_chain_snapshots"] = file_path
        elif filename.startswith("ml_features_"):
            csv_files["ml_features"] = file_path
        elif filename.startswith("exchange_metadata_"):
            csv_files["exchange_metadata"] = file_path
        elif filename.startswith("training_batches_"):
            csv_files["training_batches"] = file_path
        elif filename.startswith("vix_term_structure_"):
            csv_files["vix_term_structure"] = file_path
        elif filename.startswith("macro_signals_"):
            csv_files["macro_signals"] = file_path
        elif filename.startswith("order_book_depth_snapshots_"):
            csv_files["order_book_depth_snapshots"] = file_path
        # Skip database_summary_*.csv as it's not a table
    
    return csv_files

def clean_dataframe(df, table_name):
    """Clean and prepare dataframe for database insertion."""
    # Replace NaN with None for proper NULL handling
    df = df.where(pd.notnull(df), None)
    
    # Convert datetime columns if they exist
    datetime_columns = ['timestamp', 'created_at', 'updated_at', 'last_update_time', 
                       'start_timestamp', 'end_timestamp']
    for col in datetime_columns:
        if col in df.columns:
            # Try to parse datetime, keep as string if it fails
            try:
                df[col] = pd.to_datetime(df[col], errors='coerce')
                # Convert to string format for database
                df[col] = df[col].apply(lambda x: x.strftime('%Y-%m-%d %H:%M:%S') if pd.notnull(x) else None)
            except:
                pass
    
    # Convert boolean columns for SQLite compatibility
    if table_name == "paper_trading_metrics":
        bool_columns = ['executed', 'constraint_violation']
        for col in bool_columns:
            if col in df.columns:
                df[col] = df[col].apply(lambda x: 1 if x in (True, 1, '1', 'True', 'true') else 0 if x in (False, 0, '0', 'False', 'false') else None)
    
    return df

def import_table(conn, table_name, df, config):
    """Import a dataframe into a database table."""
    if df.empty:
        print(f"   ⚠ Skipping {table_name}: DataFrame is empty")
        return 0
    
    cursor = conn.cursor()
    ph = _get_placeholder()
    is_postgres = config.db_type == 'postgres'
    
    try:
        # Get column names from dataframe, excluding id for auto-increment tables
        # For tables with auto-increment id, we exclude it and let DB generate it
        exclude_id = table_name in [
            "option_chain_snapshots", "training_batches", "vix_term_structure",
            "macro_signals", "order_book_depth_snapshots"
        ]
        
        if exclude_id and 'id' in df.columns:
            columns = [col for col in df.columns if col != 'id']
        else:
            columns = list(df.columns)
        
        if not columns:
            print(f"   ⚠ Skipping {table_name}: No columns found")
            return 0
        
        # Prepare data rows
        rows = []
        for _, row in df.iterrows():
            row_data = tuple(row[col] for col in columns)
            rows.append(row_data)
        
        if not rows:
            print(f"   ⚠ Skipping {table_name}: No rows to insert")
            return 0
        
        # Build INSERT statement with conflict resolution
        placeholders = ', '.join([ph] * len(columns))
        
        if is_postgres:
            # PostgreSQL: Use ON CONFLICT DO UPDATE
            # Define conflict columns based on table schema
            if table_name == "ml_features":
                conflict_cols = "(timestamp, exchange)"
            elif table_name == "exchange_metadata":
                conflict_cols = "(exchange)"
            elif table_name == "option_chain_snapshots":
                conflict_cols = "(timestamp, exchange, strike, option_type)"
            else:
                # For other tables, if id exists in CSV, use it for conflict
                # Otherwise, no conflict resolution (will create duplicates)
                if 'id' in df.columns and not exclude_id:
                    conflict_cols = "(id)"
                else:
                    conflict_cols = None
            
            if conflict_cols:
                update_clause = ", ".join([f"{col} = EXCLUDED.{col}" for col in columns])
                query = f"""
                    INSERT INTO {table_name} ({', '.join(columns)})
                    VALUES ({placeholders})
                    ON CONFLICT {conflict_cols}
                    DO UPDATE SET {update_clause}
                """
            else:
                # No conflict resolution - just insert (may create duplicates)
                query = f"""
                    INSERT INTO {table_name} ({', '.join(columns)})
                    VALUES ({placeholders})
                """
        else:
            # SQLite: Use INSERT OR REPLACE
            # For SQLite, we need to handle id column differently
            if exclude_id and 'id' in df.columns:
                # For auto-increment tables, exclude id and let SQLite generate it
                query = f"""
                    INSERT OR REPLACE INTO {table_name} ({', '.join(columns)})
                    VALUES ({placeholders})
                """
            else:
                # Include all columns including id if present
                query = f"""
                    INSERT OR REPLACE INTO {table_name} ({', '.join(columns)})
                    VALUES ({placeholders})
                """
        
        # Execute batch insert
        cursor.executemany(query, rows)
        inserted_count = cursor.rowcount if hasattr(cursor, 'rowcount') and cursor.rowcount > 0 else len(rows)
        
        conn.commit()
        return inserted_count
        
    except Exception as e:
        conn.rollback()
        raise e

def import_all_data():
    """Import all CSV files from export folder into database."""
    
    config = get_config()
    db_name = config.db_name if config.db_type == 'postgres' else "db_new.db"
    
    print("=" * 70)
    print(f"CSV IMPORT TO DATABASE '{db_name}'")
    print("=" * 70)
    
    # Find CSV files
    csv_files = find_csv_files()
    
    if not csv_files:
        print(f"\n❌ No CSV files found in '{EXPORT_DIR}' directory!")
        print("   Make sure you have exported data using export_to_csv.py first.")
        return
    
    print(f"\nFound {len(csv_files)} CSV file(s) to import:")
    for table, file_path in csv_files.items():
        print(f"   - {table}: {file_path.name}")
    
    conn = None
    try:
        conn = get_db_connection()
        
        total_imported = 0
        
        # Import each table in order
        table_order = [
            "exchange_metadata",
            "vix_term_structure",
            "macro_signals",
            "order_book_depth_snapshots",
            "option_chain_snapshots",
            "ml_features",
            "training_batches"
        ]
        
        # Filter to only tables that have CSV files
        tables_to_import = [t for t in table_order if t in csv_files]
        
        for idx, table_name in enumerate(tables_to_import, 1):
            file_path = csv_files[table_name]
            print(f"\n{idx}. Importing {table_name}...")
            print(f"   Reading: {file_path.name}")
            
            try:
                # Read CSV
                df = pd.read_csv(file_path)
                print(f"   ✓ Loaded {len(df)} records from CSV")
                
                # Clean dataframe
                df = clean_dataframe(df, table_name)
                
                # Import to database
                imported_count = import_table(conn, table_name, df, config)
                total_imported += imported_count
                print(f"   ✓ Imported {imported_count} records to {table_name}")
                
            except Exception as e:
                print(f"   ❌ Error importing {table_name}: {e}")
                import traceback
                traceback.print_exc()
                continue
        
        print("\n" + "=" * 70)
        print(f"✓ IMPORT COMPLETE! Total records imported: {total_imported}")
        print("=" * 70)
        
    except Exception as e:
        print(f"\n❌ AN ERROR OCCURRED: {e}")
        import traceback
        traceback.print_exc()
    finally:
        if conn:
            release_db_connection(conn)

if __name__ == '__main__':
    import_all_data()

